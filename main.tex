\documentclass[11pt]{article}
\usepackage[letterpaper]{geometry}
\usepackage{times}
\geometry{top=1.0in, bottom=1.0in, left=1.0in, right=1.0in}
\usepackage{setspace}
\usepackage{fancyhdr}
\usepackage{fancyvrb} %Need for ventering verbatim using BVerbatim
\pagestyle{fancy}
\fancyhf{}
\rhead{Reichelt \thepage}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
%To make sure we actually have header 0.5in away from top edge
%12pt is one-sixth of an inch. Subtract this from 0.5in to get headsep value
\setlength\headsep{0.333in}


% Packages related to figures
\usepackage{wrapfig}
\usepackage{graphicx}
\usepackage{caption}
\usepackage[font=scriptsize,labelfont=bf, justification=centering]{caption}

% Text formatting packages and macros
\doublespacing
\setlength\parindent{4em}
\usepackage{ragged2e}
\setlength\RaggedRightParindent{\parindent}
\RaggedRight

% Reformat section styles
\usepackage{titlesec}
\titleformat{\section}
  {\normalfont\fontsize{11}{11}\bfseries\scshape}{\thesection}{1em}{}
\titlespacing*{\section}{0pt}{2em}{0.2em}

\titleformat{\subsection}
  {\normalfont\fontsize{9}{9}\bfseries}{\thesection}{1em}{}
\titlespacing*{\subsection}{0pt}{2em}{0.2em}

\titleformat{\subsubsection}
  {\normalfont\fontsize{9}{9}\itshape}{\thesection}{1em}{}


\newcommand{\bibent}{\noindent \hangindent 40pt}
\newenvironment{workscited}{\newpage \begin{center} Works Cited \end{center}}{\newpage }
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage[group-separator={,}]{siunitx}
\newcommand{\TODO}[1]{\textcolor{red}{[#1]}}
\newcommand{\PASS}[1]{\verb| #1|}

% Math related packages
\usepackage[fleqn]{amsmath}
\usepackage{esvect}

% Algorithm package
\usepackage{algorithmic}
    \usepackage{algorithm}

\begin{document}
{ % Header
    \thispagestyle{empty}
    \begin{flushleft}
        Scott Reichelt\\
        Dr. Scott Herring\\
        UWP104E - Section 2\\
        \today\\
    \end{flushleft}
    \vspace{1em}
}
    \begin{center}
        Review of Whale Optimization Algorithm With an Emphasis on Applications for Optimal Feature Selection in Data Analysis
    \end{center}

\section*{Abstract}
    This review traces the Whale Optimization Algorithm---a swarm intelligence meta heuristic introduced in 2016---from its origin to applications as an optimal feature selector.
    The algorithm has an intuitive model derived from observing the swarming behavior of rare groups of humpback whales who exhibit a highly cooperative hunting behavior known as bubble-net foraging; this high level intuition is presented in detail along with the underlying mathematical framework.
    The strength of the model is presented as an accumulation of related research applying the algorithm as an optimal feature selector.
    Direct applications of the original algorithm are presented along with adjustments to the algorithm such as hybridization.

\section*{Introduction} {
    Swarm algorithms are relatively easy to implement, do not necessitate knowledge of the gradient, can bypass local optima, and are adaptable for a wide range of optimization problems (Mirjalili 2016) making them a good candidate for optimization applications.
    Meta-heuristic approaches appeared in the literature in the 1960s with algorithms modeled on evolution, while algorithms modeled on swarm intelligence first appeared in 1992 with Marco Dorigo's Ant Colony Optimization (Wahab et al. 2015).
    The Whale Optimization Algorithm (WOA) is the most recent swarm based meta-heuristic algorithm, and has quickly proved itself as a competitive general purpose optimization approach (Mirjalili 2016). 
    This review first provides a summary of the WOA, then presents applications of WOA as an optimal feature selector.
}

\section*{The Whale Optimization Algorithm} {
    The WOA draws its inspiration from a hunting behavior observable in rare populations of humpback whales called bubble-net feeding.
    The technique allows groups of whales to efficiently corral and consume small fish (Wiley et al. 2011).
    The whales exhibit swarming behavior both when they seek out potential prey and with the cooperative bubble-net attack pattern.
    The hunting behavior provides natural analogs to both the exploration phase (seeking prey) and the exploitation phase (bubble-net) of swarm algorithms.
}

\subsection*{Bubble-net Foraging: Cetological Background} {
    The hunting behavior of interest involves a swarm of humpback whales finding a target school of fish, encircling it, then cooperatively coralling the prey into a tight area with a single whale executing the bubble-net.
    The bubble-netting whale dives below the target prey and encircles it with a column of bubbles purposefully emanating from its blow hole (Wiley et al. 2011).
    The bubbles form an impassable barrier (Wiley et al. 2011) for the fish, and concentric circles corral target prey into a shrinking area through either an upward-spiral or double loop maneuver (Wiley et al. 2011).
    The ring of bubbles forming the bubble-net can be seen from the surface of the water (Figure 1), and as the column of bubbles corrals prey the team of humpback whales lunge from the columns center to feed as seen in Figure 2.

    \begin{figure}[h]
        \centering
        \caption{Bubble spiral aerial view courtesy of NOAA}
        \includegraphics[width=\textwidth]{spiral.jpg}
    \end{figure}

    \begin{figure}[h]
        \centering
        \caption{Humpback whales lunging through the center of a completed bubble net}
        \includegraphics[width=\textwidth]{lunge.jpg}
    \end{figure}
}
\subsection*{Mathematically Modeling the Bubble-Net Behavior} {
    In the exploration phase of the algorithm whales are dispersed throughout the search space using a stochastic process, and will iteratively migrate towards the current best candidate solution as posited by Mirjalili in his originating 2016 work according to equations 1--4.

    \begin{equation}
        \vec{D} = |\vec{C}\cdot\vec{X}^*(t) - \vec{X}(t)|
    \end{equation}

    \begin{equation}
        \vec{X}(t + 1) = |\vec{X}(t) - \vec{A}\cdot\vec{D})|
    \end{equation}

    \begin{equation}
        \vec{A} = 2\vec{a}\cdot\vec{r}-\vec{a}
    \end{equation}

    \begin{equation}
        \vec{C} = 2\cdot\vec{r}
    \end{equation}

    Where $\cdot$ is element by element multiplication, $| \;\; |$ is the absolute value, $\vec{X}^*$ is the current best solution, $\vec{A}$ and $\vec{C}$ are coefficient vectors, and $t$ denotes the current iteration.
    The vector $\vec{a}$ decreases linearly from $2$ to $0$, which gradually shrinks the search space, while $\vec{r}$ is a random vector in [$0,1]$that ensures all points in the search space are reachable.
    Equations 1--4 model the basic encircling of prey preformed by the swarm of whales and easily extends to n-dimensional space where the search agents occupy a hypercube (Mirjalili 2016).

    The exploration phase of the algorithm relies on stochastic principles by using a random search agent in place of the best according to equations 5--6.

    \begin{equation}
        \vec{D} = |\vec{C}\cdot\vec{X}_{rand} - \vec{X}|
    \end{equation}

    \begin{equation}
        \vec{X}(t+1) = \vec{X}_{rand} - \vec{A}\cdot\vec{D}
    \end{equation}

    The exploitation phase of the algorithm also uses a stochastic process to improve convergence by randomly using either the spiral or double loop attack pattern on a candidate solution.
    Equation 7 introduces a spiral pattern and a mechanism to randomly switch between the spiral and encircling behaviors.

    \begin{equation}
        \vec{X}(t+1) =
        \begin{cases}
            \vec{X}^*(t) - \vec{A}\cdot\vec{D} &if \; p < 0.5\\
            \vec{D}'\cdot e^{bl} \cdot cos(2\pi l) + \vec{X}^*(t) &if \; p \geq 0.5\\
        \end{cases}
    \end{equation}

    Where $b$ is a constant for the logarithmic spiral and $l$ is a random number in $[-1, 1]$ (Mirjalili 2016).

    \begin{figure}
        \caption{WOA pseudo code (Mirjalili \& Lewis 2016)}
        \begin{algorithm}[H]
            \algsetup{linenosize=\tiny}
            \scriptsize
            \begin{algorithmic}[1]
                \STATE Initialize the whales population $X_i(i = 1,2, \dots, n)$
                \STATE Calculate the fitness of each search agent
                \STATE $X^* = $ the best search agent
                \WHILE {$t <$ maximum number of iterations}
                    \FOR {each search agent}
                        \STATE update a, A, C, l, and p
                        \IF {$p < 0.5$}
                            \IF {$|A| < 1$}
                                \STATE Update the position of the current search agent by Equation 1
                            \ELSE
                                \STATE Select a random search agent ($X_{rand}$)
                                \STATE Update the position of the current search agent by Equation 6
                            \ENDIF
                        \ELSE
                            \STATE Update the position of the current search by Equation 7
                        \ENDIF
                    \ENDFOR
                    \STATE Check if any search agent goes beyond the search space and amend it
                    \STATE Calculate the fitness of each search agent
                    \STATE Update $X^*$ if there is a better solution
                    \STATE $t = t+1$
                \ENDWHILE
                \STATE \RETURN $X^*$ 
            \end{algorithmic}
        \end{algorithm}
    \end{figure}

\subsection*{Algorithm Implementation and Benchmark Results} {
    The algorithm is demonstratively competitive with both canon swarm algorithms---ie Particle Swarm Optimization---as well as more mature classes of meta-heuristics like Evolutionary algorithms (Mirjalili 2016).
    In the initial benchmarks Mirjalili found the WOA converged either most or second most efficiently in 29 tests comparing representative meta-heuristics like Particle Swarm Optimization, Gravitational Search Algorithm, and Differential Evolution Algorithm.
    The transition between exploration and exploitation is facilitated by $\vec{a}$ gradually reducing in size (thus shrinking the search space), and the entire process leverages random chance through $p$ and $X_{rand}$ (Mirjalili 2016).
    The Algorithms combination of gradually shrinking the search space while exploring towards a random search agent gives it a strong exploration phase.
    The exploitation phase also utilizes random chance to give search agents opportunities to explore more of the search space while moving towards the candidate solution, aiding the algorithm in avoiding local optima.
    Figure 3 contains the entire pseudo-code for WOA, and its relative simplicity combined with competitive benchmarks lend to its appeal both in application directly and the potential to further improve its convergence rate through standard meta heuristic techniques.
}

\section*{Dimensionality Reduction Using WOA as an Optimal Feature Selector} {
    Since its 2016 introduction WOA has seen use in many optimization problems in engineering, but the remainder of this review will focus on applications that employ WOA as an optimal feature selector.
    In applications of data mining optimal feature selection can improve computation speeds by removing irrelevant features or selecting the strongest predicting subset of features (Hussien et al. 2017).
    The ability of the WOA to reach the entire search space using either a random or the best agent while avoiding converging towards local minima (Sharawi et al. 2017) make WOA a natural choice for this type of optimization problem.

\subsection*{Methods for Using WOA as an Optimal Feature Selector} {
    The WOA can be directly applied using a wrapper-based method for feature selection as Sayed et al. did with a breast cancer diagnostic machine learning (2017) as well as Emary and Zawbaa who employ the k-nearest neighbor (KNN) classifier to benchmark WOA against several other algorithms using 16 test datasets (2017).
    In the machine learning breast cancer diagnosis problem, WOA selects features that are fed into a support vector machine (SVM) and the results are benchmarked against other approaches commonly used for this application (Sayed et al. 2017).
    Both applications do not alter the WOA from the original 2016 publication.
    Hybrid meta-heuristics are a common way of finding incremental improvements of optimization algorithms for specific applications, Mafarja and Mirjalili explore this direction by combining WOA with Simulated Annealing (SA) with the feature selection problem.
    The WOA exploration phase dispropotiontly contributes to its efficacy (Mirjalili 2016), thus a hybrid meta-heuristic which uses the WOA exploration phase with SA exploitation phase is explored (Mafarja \& Mirjalili 2017).
    Hussien et al. use a hyperbolic tangent function to construct an altered WOA that maps continuous values to binary values to improve fitness calculations in the context of optimal feature selection (2017).
}

\subsection*{Results of WOA as an Optimal Feature Selector} {
    All four papers found WOA to be effective for the problem.
    For breast cancer diagnosing WOA achieved 98.7\% accuracy with 99.15\% precision, outperforming the currently used approaches as seen in Figure 4 (Sayed et al. 2017).
    This result was expected, but is still significant in that it demonstrates that---with no adjustments in a application environment---WOA preforms better then competing methods, and the result is duplicated on a larger scale with Emary and Zawbaa.
    What is significant is that these approaches made no adjustments to WOA, but other research demonstrates that the algorithm can be altered for feature selection in a way that improves its performance.
    With a hybrid meta-heuristic approach combining SA, WOA, and tournament selection Mafarja and Mirjalili preform a benchmarking procedure very similar to Emary and Zawbaa, but include the original WOA as a competing approach (2017).
    The resulting data demonstrates that the hybrid approach outperforms the original WOA algorithm as well as the other representative techniques.
    The hyperbolic tangent function method is tested against the original WOA and outperforms that algorithm (Hessien et al. 2017).
    The growing body of research demonstrating WOA as a high competitive optimizer is made even more promising given the history of optimizers incrementally improving over time.
    While WOA is already a strong choice for applications, it has room to improve.
}
\begin{workscited}

    \bibent
    Sayed G.I., Darwish A., Hassanien A.E., Pan JS. 2017. Breast Cancer Diagnosis Approach Based on Meta-Heuristic Optimization Algorithm Inspired by the Bubble-Net Hunting Strategy of Whales. Pan, JS; Lin, JCW; Wang, CH; Jiang, XH. 10th International Conference on Genetic and Evolutionary Computing (ICGEC); NOV 07-09, 2016; Fujian Univ Technol, Fuzhou, PEOPLES R CHINA. Springer, Cham; [accessed 2018 Feb 20]. 306--313. \url{https://link.springer.com/chapter/10.1007/978-3-319-48490-7_36}.\url{doi:org/10.1007/978-3-31948490-7_36}.

    \bibent
    Mirjalili, S. 2016. The Whale Optimization Algorithm. Advances in Engineering Software. [accessed 2018 Feb 18]; 95: 51--67. \url{https://www.sciencedirect.com/science/article/pii/S0965997816300163} \url{doi:org/10.1016/j.advengsoft.2016.01.008}

    \bibent
    Ab Wahab MN, Nefti-Meziani S, Atyabi A. 2015. A Comprehensive Review of Swarm Optimization Algorithms. PLoS ONE. [accessed 2018 Feb 21]; 10(5): e0122827. \url{http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0122827} \url{doi:org/10.1371/journal.pone.0122827}

    \bibent
    Wiley D. 2011. Underwater components of humpback whale bubble-net feeding behaviour. Behaviour. [accessed 2018 Feb 19]; 148(5-6), 575--602. \url{http://booksandjournals.brillonline.com/content/journals/10.1163/000579511x570893}.\url{doi.10.1163/000579511X570893}.

    \bibent
    Mafarja M \& Mirjalili, S. 2017. Hybrid Whale Optimization Algorithm with Simulated Annealing for Feature Selection. Neurocomputing. [accessed 2018 Feb 21]; 260; 302-312 \url{https://www.researchgate.net/publication/316895153_Hybrid_Whale_Optimization_Algorithm_with_Simulated_Annealing_for_Feature_Selection}. \url{doi:10.1016/j.neucom.2017.04.053}.

    \bibent
    Sharawi M, Zawbaa HM, and Emary E. 2017. Feature selection approach based on whale optimization algorithm. Ninth International Conference on Advanced Computational Intelligence (ICACI); 2017 Feb 4-6; Doha, Qatar. [accessed 2017 Feb 22] pp. 163-168. \url{http://ieeexplore.ieee.org/document/7974502/?anchor=authors}. \url{doi: 10.1109/ICACI.2017.7974502}.
\end{workscited}

\end{document}
\}
